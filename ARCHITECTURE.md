# System Architecture & Design Decisions

## ğŸ›ï¸ Architecture Overview

This system implements a **production-grade ETL (Extract, Transform, Load) pipeline** with a RESTful API for querying cryptocurrency data. The architecture follows best practices for reliability, scalability, and maintainability.

## ğŸ¯ Design Principles

### 1. Separation of Concerns
- **Data Sources** (`ingestion/sources/`) - External API/file interactions
- **Transformation** (`ingestion/transformer.py`) - Business logic
- **Storage** (`ingestion/loader.py`) - Database operations
- **API** (`api/`) - HTTP interface
- **Core** (`core/`) - Configuration and shared utilities

### 2. Idempotency
All operations are designed to be safely re-runnable:
- **Upsert logic** in database loader
- **Checkpoint tracking** prevents duplicate processing
- **Deterministic transformations** produce same output for same input

### 3. Fault Tolerance
- **Retry logic** with exponential backoff
- **Checkpoint system** for resume-on-failure
- **Error isolation** - one source failure doesn't block others
- **Graceful degradation** - system continues with partial data

### 4. Observability
- **Structured logging** with timestamps and context
- **Health endpoints** for monitoring
- **Statistics tracking** for each ETL run
- **Request tracing** with unique request IDs

## ğŸ“Š Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA SOURCES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ CoinPaprika  â”‚  â”‚  CoinGecko   â”‚  â”‚   CSV File   â”‚     â”‚
â”‚  â”‚   REST API   â”‚  â”‚   REST API   â”‚  â”‚  sample.csv  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚ Rate Limiting    â”‚ Rate Limiting    â”‚
          â”‚ + Retries        â”‚ + Retries        â”‚ CSV Parse
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTRACTION LAYER                          â”‚
â”‚                  (Pydantic Validation)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RawCoinPaprika  â”‚  RawCoinGecko   â”‚   RawCSV              â”‚
â”‚     Schema       â”‚     Schema      â”‚    Schema              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                  â”‚
          â”‚                â”‚                  â”‚
          â–¼                â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAW DATA STORAGE                           â”‚
â”‚                    (PostgreSQL)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  raw_coinpaprika â”‚  raw_coingecko  â”‚   raw_csv             â”‚
â”‚  Full JSON logs  â”‚  Full JSON logs â”‚   CSV records         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                  â”‚
          â”‚                â”‚                  â”‚
          â–¼                â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 TRANSFORMATION LAYER                         â”‚
â”‚            (DataTransformer + Validation)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Transform to UnifiedCryptoDataSchema                       â”‚
â”‚  - Normalize field names                                    â”‚
â”‚  - Convert types                                            â”‚
â”‚  - Add source metadata                                      â”‚
â”‚  - Validate with Pydantic                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  UNIFIED DATA STORAGE                        â”‚
â”‚         (PostgreSQL with Upsert/Idempotent Writes)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              unified_crypto_data table                       â”‚
â”‚  - coin_id, name, symbol                                    â”‚
â”‚  - price_usd, market_cap, volume_24h                        â”‚
â”‚  - source, ingested_at                                      â”‚
â”‚                                                              â”‚
â”‚  Indexes:                                                    â”‚
â”‚  - (coin_id, source, ingested_at) for upsert               â”‚
â”‚  - coin_id for lookups                                      â”‚
â”‚  - source for filtering                                     â”‚
â”‚  - ingested_at for time-based queries                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API LAYER                               â”‚
â”‚                    (FastAPI)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  GET /api/v1/data                                           â”‚
â”‚  â”œâ”€ Pagination (page, page_size)                           â”‚
â”‚  â”œâ”€ Filtering (source, coin_id)                            â”‚
â”‚  â”œâ”€ Request tracking (request_id)                          â”‚
â”‚  â””â”€ Latency measurement (api_latency_ms)                   â”‚
â”‚                                                              â”‚
â”‚  GET /api/v1/health                                         â”‚
â”‚  â”œâ”€ Database connectivity check                            â”‚
â”‚  â””â”€ Last ETL run status                                    â”‚
â”‚                                                              â”‚
â”‚  GET /api/v1/stats                                          â”‚
â”‚  â”œâ”€ Records processed per source                           â”‚
â”‚  â”œâ”€ Duration per source                                    â”‚
â”‚  â””â”€ Success/failure timestamps                             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ ETL Checkpoint System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CHECKPOINT LIFECYCLE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. START RUN
   â”œâ”€ Check if source is already running
   â”‚  â””â”€ If yes: Skip (prevent concurrent runs)
   â”œâ”€ Create/Update checkpoint
   â”‚  â”œâ”€ source: "coinpaprika"
   â”‚  â”œâ”€ status: "running"
   â”‚  â””â”€ last_run_timestamp: NOW
   â””â”€ Commit to database

2. EXECUTE ETL
   â”œâ”€ Extract data from source
   â”œâ”€ Validate with Pydantic
   â”œâ”€ Load raw data
   â”œâ”€ Transform to unified schema
   â””â”€ Load unified data (upsert)

3. COMPLETE RUN (Success)
   â”œâ”€ Update checkpoint
   â”‚  â”œâ”€ status: "success"
   â”‚  â”œâ”€ records_processed: COUNT
   â”‚  â”œâ”€ duration_seconds: ELAPSED
   â”‚  â””â”€ error_message: NULL
   â””â”€ Commit to database

3. COMPLETE RUN (Failure)
   â”œâ”€ Update checkpoint
   â”‚  â”œâ”€ status: "failure"
   â”‚  â”œâ”€ records_processed: 0
   â”‚  â”œâ”€ duration_seconds: ELAPSED
   â”‚  â””â”€ error_message: ERROR_DETAILS
   â””â”€ Commit to database

4. RESUME FROM CHECKPOINT
   â”œâ”€ Query last successful run
   â”œâ”€ Determine what to re-process
   â””â”€ Continue from last good state
```

## ğŸ” Error Handling Strategy

### Retry Logic (Exponential Backoff)

```python
Attempt 1: Wait 1.0s    (initial_delay)
Attempt 2: Wait 2.0s    (delay * 2)
Attempt 3: Wait 4.0s    (delay * 2)
Attempt 4: Wait 8.0s    (delay * 2)
...
Max Wait:  60.0s        (max_delay cap)

Triggers:
- HTTP 429 (Rate Limit)
- HTTP 500, 502, 503, 504 (Server Errors)
- Connection Timeout
- Connection Refused
```

### Error Isolation

```
Source A fails â†’ Checkpoint marked as "failure"
                â†“
Source B runs   â†’ Independent execution
                â†“
Source C runs   â†’ Independent execution
                â†“
API still works â†’ Returns data from B and C
```

## ğŸ’¾ Database Design Decisions

### Why Separate Raw and Unified Tables?

**Raw Tables:**
- **Preserve original data** for auditing
- **Debug data quality issues** at source
- **Re-transform** if business logic changes
- **Track API changes** over time

**Unified Table:**
- **Consistent schema** across all sources
- **Query performance** - single table for API
- **Business logic layer** - clean separation
- **Easy to extend** - add new sources without breaking API

### Why PostgreSQL?

1. **ACID Compliance** - Data integrity guarantees
2. **Advanced Features:**
   - `INSERT ... ON CONFLICT` for upserts
   - JSON column type for raw data storage
   - Partial indexes for performance
3. **Production Battle-Tested** - Proven at scale
4. **Rich Ecosystem** - Monitoring, backups, replication

### Indexing Strategy

```sql
-- Fast lookups by coin
CREATE INDEX idx_unified_coin_id ON unified_crypto_data(coin_id);

-- Filter by source
CREATE INDEX idx_unified_source ON unified_crypto_data(source);

-- Time-based queries
CREATE INDEX idx_unified_ingested ON unified_crypto_data(ingested_at);

-- Composite for upsert uniqueness
CREATE UNIQUE INDEX idx_unified_coin_source 
ON unified_crypto_data(coin_id, source, ingested_at);
```

## ğŸš¦ Rate Limiting Implementation

### Two-Layer Protection

**1. Request Delay (Rate Limiting)**
```python
make_request()
sleep(0.5s)  # RATE_LIMIT_DELAY
```
Prevents overwhelming external APIs with requests.

**2. Exponential Backoff (Error Recovery)**
```python
try:
    make_request()
except RateLimitError:
    wait(delay)
    delay *= 2  # Exponential increase
    retry()
```
Handles 429 responses gracefully.

## ğŸ” Observability & Monitoring

### Logging Strategy

```python
# Structured logging with context
logger.info(
    "Completed ETL run",
    extra={
        "source": "coinpaprika",
        "records": 100,
        "duration": 5.23,
        "status": "success"
    }
)
```

### Metrics Tracked

**Per ETL Run:**
- Records processed
- Duration (seconds)
- Success/failure status
- Error messages
- Timestamp

**Per API Request:**
- Request ID (UUID)
- Latency (milliseconds)
- Results count
- Filters applied

## ğŸ³ Docker Architecture

### Multi-Stage Build Benefits

**Stage 1: Builder**
- Installs build dependencies (gcc, etc.)
- Compiles Python packages
- Creates wheel files

**Stage 2: Runtime**
- Minimal base image
- Only runtime dependencies
- Smaller final image (security + performance)

**Result:**
- Build image: ~800MB
- Final image: ~300MB (60% reduction)

### Service Dependencies

```yaml
postgres:
  â†“ (health check: pg_isready)
api:
  â†“ (waits for postgres healthy)
  â”œâ”€ Run ETL pipeline
  â””â”€ Start FastAPI server
```

## ğŸ“ˆ Scalability Considerations

### Current Design Supports:

1. **Horizontal Scaling**
   - Stateless API (multiple instances)
   - Read replicas for PostgreSQL
   - Load balancer in front

2. **Vertical Scaling**
   - Increase PostgreSQL resources
   - More API container memory
   - Larger connection pools

3. **Data Volume**
   - Partitioning by date (ingested_at)
   - Archiving old raw data
   - Materialized views for aggregations

### Future Enhancements:

- Task queue (Celery) for ETL scheduling
- Caching layer (Redis) for API responses
- Message queue (RabbitMQ) for real-time updates
- Data warehouse (Snowflake) for analytics

## ğŸ§ª Testing Philosophy

### Test Pyramid

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â•±  Integration â•²      â† Few, test API + DB
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â•±   Component      â•²    â† Some, test ETL flow
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â•±        Unit           â•² â† Many, test transformations
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Unit Tests** - Fast, isolated, pure functions  
**Integration Tests** - API + test database  
**Recovery Tests** - Checkpoint logic, retry mechanisms  

## ğŸ“ Key Takeaways

âœ… **Idempotency** - Safe to re-run at any time  
âœ… **Fault Tolerance** - Graceful error handling  
âœ… **Observability** - Logging, metrics, health checks  
âœ… **Separation of Concerns** - Clear module boundaries  
âœ… **Production-Ready** - Docker, tests, documentation  
âœ… **Scalable Design** - Ready for growth  
âœ… **Type Safety** - Pydantic validation everywhere  
âœ… **Clean Code** - PEP 8, type hints, docstrings  

This architecture demonstrates enterprise-level software engineering practices suitable for production cryptocurrency data systems.
